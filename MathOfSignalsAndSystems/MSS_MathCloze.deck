# Number; Cloze phrase; Reference
# Chapter 2
CH2_01; A {{c1::metric}} \(d: X \times X \rightarrow \mathbb{R}\) is a function that is used to {{c2::measure distance}} between elements in a set X.; Defintion 2.1, pg 72
CH2_02; For a metric \(d: X \times X \rightarrow \mathbb{R}\), and for all \(x, \: y \in X\),<br> {{c1:: \(d(x, \: y) \) }} \(=\) {{c2:: \(d(y, \: x) \) }}; M1, pg 72
CH2_03; For a metric \(d: X \times X \rightarrow \mathbb{R}\), and for all \(x, \: y \in X\),<br> \(d(x, \: y) \) \(\geq\) {{c1:: \(0\)}}; M2, pg 72
CH2_04; For a metric \(d: X \times X \rightarrow \mathbb{R}\), and for all \(x, \: y \in X\),<br> \(d(x, \: y) \) \(=\) \(0\) {{c1::if and only if}} {{c2::\(x = y\)}}; M3, pg 72
CH2_05; For a metric \(d: X \times X \rightarrow \mathbb{R}\), and for all \(x, \: y, \: z \in X\),<br> {{c1::\(d(x, \: z) \)}} \(\leq\) {{c2::\(d(x, \: y) + d(y, \: z)\)}}<br> This is also known as the {{c3::triangle inequality}}; M4, pg 72
CH2_06; Let \(\mathbf{x} \in\) {{c2::\(\mathbb{R}^n\)}} and \(\mathbf{y} \in\) {{c2:: \(\mathbb{R}^n\)}}. The {{c1::\(l_p\)}} metric is <br><br> \(d_p(\mathbf{x}, \: \mathbf{y}) = \) {{c3:: \( \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p}\) }}; Example 2.1.2, part 3, pg 73
CH2_07; Let \(\mathbf{x} \in\) {{c2::\(\mathbb{R}^n\)}} and \(\mathbf{y} \in\) {{c2::\(\mathbb{R}^n\)}}. The {{c1::\(l_{\infty}\)}} metric is <br><br> \(d_\infty(\mathbf{x}, \: \mathbf{y}) = \) {{c3:: \( \max_{i=1,\:2,\dots,\:n} |x_i - y_i| \) }}; Example 2.1.2, part 4, pg 73
CH2_08; A {{c1::metric space}} \((X, d)\) is a {{c2::set \(X\)}} together with a {{c2::metric \(d\)}}.; Definition 2.2, pg 74
CH2_09; For a set \(S \subset \mathbb{R}\), the {{c1::least upper bound (LUB)}} is the {{c2::smallest number \(z\)}} such that {{c3::\(z \geq x\) for every \(x \in S\) }}. The {{c1::LUB}} of a set \(S\) is called the {{c1::supremum}} and is denoted {{c1::\( \sup \)}}; Box 2.1, pg 74
CH2_10; For a set \(S \subset \mathbb{R}\), the {{c1::greatest lower bound (GLB)}} is the {{c2::largest number \(z\)}} such that {{c3::\(z \leq x\) for every \(x \in S\) }}. The {{c1::GLB}} of a set \(S\) is called the {{c1::infinum}} and is denoted {{c1::\( \inf \)}}; Box 2.1, pg 74
CH2_11; If for every \(\delta > 0\), there is an \(n_0\) such that {{c1::\(d(x_n, x^*) < \delta\)}} for every {{c2::\(n > n_0\)}} for some {{c3::fixed value}} \(x^*\), then the sequence \(\{x_n\}\) is said to {{c4::converge to}} \(x^*\).; Definition 2.10, pg 79
CH2_12; If the sequence \(\{x_n\}\) converges to \(x^*\), we write {{c1::\(x_n \rightarrow x^* \)}}, and we say that \(x^*\) is the {{c2::limit}} of \(x_n\).; Definition 2.10, pg 79
CH2_13; If the sequence \(\{x_n\}\) {{c1::returns infinitely often}} to a {{c1::neighborhood}} of a point \(x^*\), say that \(x^*\) is a {{c2::limit point}}.; Definition 2.10, pg 79
CH2_14; A sequence \(\{x_n\}\) in a metric space \( (X, d) \) is said to be a {{c1::Cauchy sequence}} if, for any \(\epsilon > 0\), there is an \( N > 0\) (which may depend on \(\epsilon\)) such that {{c2::\(d(s_n, x_m) < \epsilon \) }} for every {{c2::\(m,\:n > N\)}}.; Definition 2.12, pg 80
CH2_15; A metric space \((X, d)\) is {{c1::complete}} if every {{c2::Cauchy sequence}} in \(X\) is {{c2::convergent in}} \(X\).; Definition 2.13, pg 81
# chapter 3
CH3_01; A matrix \(A\) is said to be positive definite (PD) if {{c1::\(\mathbf{x}^H A \mathbf{x}\ > 0\)}} for all \(\mathbf{x} \neq 0 \); Definition 3.1, pg 134
CH3_02; A matrix \(A\) is said to be positive semidefinite (PSD) if {{c1::\(\mathbf{x}^H A \mathbf{x}\ \geq 0\)}} for all \(\mathbf{x} \neq 0 \); Definition 3.1, pg 134
CH3_03; All diagonal elements of a positive definite (or PSD) are {{c1::nonnegative}}; Definition 3.1, pg 134
CH3_04; A Hermitian matrix \(A\) is PD (or PSD) if and only if all of the eigenvalues are {{c1::nonnegative}}. Hence, a PD matrix has a {{c1::positive determinant}}. Hence, a PD matrix is {{c1::invertable}}; Definition 3.1, pg 134
CH3_05; A Hermitian matrix \(A\) is PD if and only if all principal minors are {{c1::positive}}.; Definition 3.1, pg 134
CH3_06; If \(A\) is PD, then the pivots obtained in the {{c1::LU}} factorization are {{c1::positive}}; Definition 3.1, pg 134
CH3_07; If \(A > 0\) and \(B \geq 0\) (ie \(A\), \(B\) are PD), then \(A + B {{c1:: > 0 }} \).; Definition 3.1, pg 134
CH3_08; A {{c1::Hermitian}} PD matrix \(A\) can be factored as {{c1::\(A = B^H B\)}}, where \(B\) is full rank. This is a matrix {{c1::square root}}; Definition 3.1, pg 134
CH3_09; A Grammian matrix \(R\) is always {{c2::positive-semidefinite}}. It is {{c2::positive-definite}} if and only if the vectors \(\mathbf{p}_1\), \(\dots\), \(\mathbf{p}_m\) are {{c1::linearly independent}}.; Theorem 3.1, pg 134
CH3_10; The {{c1::orthogonality}} principle. Let \(\mathbf{p}_1\), \(\dots\), \(\mathbf{p}_m\) be data vectors in a vector space \(S\). Let \(\mathbf{x} \in S\). In the representation \[\mathbf{x} = \sum_{i=1}^{m} c_i \mathbf{p}_i + \mathbf{e} = \hat{\mathbf{x}} + \mathbf{e},\] the induced norm of the error vector \(\| \mathbf{e} \|\) is minimized when the error \( \mathbf{e} = \mathbf{x} - \hat{\mathbf{x}} \) is {{c1::orthogonal}} to each of the data vectors. i.e. \[ \left\langle \mathbf{x} - \sum_{i=1}^{m} c_i \mathbf{p}_i , \mathbf{p}_j \right\rangle = 0\] for \(j = 1,2,\dots, m\).; Theorem 3.2, pg 135
CH3_11; The optimal (least-squares) coefficients \(\mathbf{c}\) are \[\mathbf{c} = {{c1:: (A^HA)^{-1}A^H }} \mathbf{x}\].; Equation 3.19, pg 139


