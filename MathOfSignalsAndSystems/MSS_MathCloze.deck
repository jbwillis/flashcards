# Number; Cloze phrase; Reference
# Chapter 2
MSS_CH2_01; A {{c1::metric}} \(d: X \times X \rightarrow \mathbb{R}\) is a function that is used to {{c2::measure distance}} between elements in a set X.; Defintion 2.1, pg 72
MSS_CH2_02; For a metric \(d: X \times X \rightarrow \mathbb{R}\), and for all \(x, \: y \in X\),<br> {{c1:: \(d(x, \: y) \) }} \(=\) {{c2:: \(d(y, \: x) \) }}; M1, pg 72
MSS_CH2_03; For a metric \(d: X \times X \rightarrow \mathbb{R}\), and for all \(x, \: y \in X\),<br> \(d(x, \: y) \) \(\geq\) {{c1:: \(0\)}}; M2, pg 72
MSS_CH2_04; For a metric \(d: X \times X \rightarrow \mathbb{R}\), and for all \(x, \: y \in X\),<br> \(d(x, \: y) \) \(=\) \(0\) {{c1::if and only if}} {{c2::\(x = y\)}}; M3, pg 72
MSS_CH2_05; For a metric \(d: X \times X \rightarrow \mathbb{R}\), and for all \(x, \: y, \: z \in X\),<br> {{c1::\(d(x, \: z) \)}} \(\leq\) {{c2::\(d(x, \: y) + d(y, \: z)\)}}<br> This is also known as the {{c3::triangle inequality}}; M4, pg 72
MSS_CH2_06; Let \(\mathbf{x} \in\) {{c2::\(\mathbb{R}^n\)}} and \(\mathbf{y} \in\) {{c2:: \(\mathbb{R}^n\)}}. The {{c1::\(l_p\)}} metric is <br><br> \(d_p(\mathbf{x}, \: \mathbf{y}) = \) {{c3:: \( \left( \sum_{i=1}^n |x_i - y_i|^p \right)^{1/p}\) }}; Example 2.1.2, part 3, pg 73
MSS_CH2_07; Let \(\mathbf{x} \in\) {{c2::\(\mathbb{R}^n\)}} and \(\mathbf{y} \in\) {{c2::\(\mathbb{R}^n\)}}. The {{c1::\(l_{\infty}\)}} metric is <br><br> \(d_\infty(\mathbf{x}, \: \mathbf{y}) = \) {{c3:: \( \max_{i=1,\:2,\dots,\:n} |x_i - y_i| \) }}; Example 2.1.2, part 4, pg 73
MSS_CH2_08; A {{c1::metric space}} \((X, d)\) is a {{c2::set \(X\)}} together with a {{c2::metric \(d\)}}.; Definition 2.2, pg 74
MSS_CH2_09; For a set \(S \subset \mathbb{R}\), the {{c1::least upper bound (LUB)}} is the {{c2::smallest number \(z\)}} such that {{c3::\(z \geq x\) for every \(x \in S\) }}. The {{c1::LUB}} of a set \(S\) is called the {{c1::supremum}} and is denoted {{c1::\( \sup \)}}; Box 2.1, pg 74
MSS_CH2_10; For a set \(S \subset \mathbb{R}\), the {{c1::greatest lower bound (GLB)}} is the {{c2::largest number \(z\)}} such that {{c3::\(z \leq x\) for every \(x \in S\) }}. The {{c1::GLB}} of a set \(S\) is called the {{c1::infinum}} and is denoted {{c1::\( \inf \)}}; Box 2.1, pg 74
MSS_CH2_11; If for every \(\delta > 0\), there is an \(n_0\) such that {{c1::\(d(x_n, x^*) < \delta\)}} for every {{c2::\(n > n_0\)}} for some {{c3::fixed value}} \(x^*\), then the sequence \(\{x_n\}\) is said to {{c4::converge to}} \(x^*\).; Definition 2.10, pg 79
MSS_CH2_12; If the sequence \(\{x_n\}\) converges to \(x^*\), we write {{c1::\(x_n \rightarrow x^* \)}}, and we say that \(x^*\) is the {{c2::limit}} of \(x_n\).; Definition 2.10, pg 79
MSS_CH2_13; If the sequence \(\{x_n\}\) {{c1::returns infinitely often}} to a {{c1::neighborhood}} of a point \(x^*\), say that \(x^*\) is a {{c2::limit point}}.; Definition 2.10, pg 79
MSS_CH2_14; A sequence \(\{x_n\}\) in a metric space \( (X, d) \) is said to be a {{c1::Cauchy sequence}} if, for any \(\epsilon > 0\), there is an \( N > 0\) (which may depend on \(\epsilon\)) such that {{c2::\(d(s_n, x_m) < \epsilon \) }} for every {{c2::\(m,\:n > N\)}}.; Definition 2.12, pg 80
MSS_CH2_15; A metric space \((X, d)\) is {{c1::complete}} if every {{c2::Cauchy sequence}} in \(X\) is {{c2::convergent in}} \(X\).; Definition 2.13, pg 81
# vector spaces
MSS_CH2_16; A {{c1::linear vector space}} \(S\) over a set of scalars \(R\) is a collection of objects known as {{c1::vectors}}, together with an {{c2::additive operation}} and a scalar {{c2::multiplication operation}}.; Definition 2.14, pg 85
MSS_CH2_17; For a linear vector space \(S\) over a set of scalars \(R\), \(S\) forms a group under {{c1::addition}}.; VS1, pg 85 
MSS_CH2_18; For a linear vector space \(S\) over a set of scalars \(R\), for any \(\mathbf{x}, \: \mathbf{y} \in S \), \(\mathbf{x} + \mathbf{y}\) {{c1::\(\in S\)}}. <br> (The addition operation is closed.); VS1 (a), pg 85
MSS_CH2_19; For a linear vector space \(S\) over a set of scalars \(R\), there is an {{c1::additive identity}} element in \(S\), which is denoted as {{c2::\( \mathbf{0} \) }}, such that for any \(\mathbf{x} \in S\), <br>\(\mathbf{x} + \mathbf{0} = \) {{c2:: \(\mathbf{0} + \mathbf{x} = \mathbf{x} \)}}; VS1 (b), pg 85
MSS_CH2_20; For a linear vector space \(S\) over a set of scalars \(R\), for every element \(\mathbf{x} \in S\), there is another element \(\mathbf{y} \in S\) such that {{c1::\(\mathbf{x} + \mathbf{y} = \mathbf{0} \)}}. The element \(\mathbf{y}\) is the {{c2::additive inverse}} of \(\mathbf{x}\) and is usually denoted as \(-\mathbf{x}\); VS1 (c), pg 85
MSS_CH2_21; For a linear vector space \(S\) over a set of scalars \(R\), the addition operation is {{c1::associative}}, that is for any \(\mathbf{x}, \: \mathbf{y}, \: \mathbf{z} \in S \), <br> {{c2:: \( (\mathbf{x} + \mathbf{y}) + \mathbf{z} = \mathbf{x} + (\mathbf{y} + \mathbf{z}) \)}}; VS1 (d), pg 85
MSS_CH2_22; For a linear vector space \(S\) over a set of scalars \(R\), for any \(a \in R\), and for any \(\mathbf{x} \in S \), <br> \(a\mathbf{x}\) {{c1::\(\in S\)}}; VS2, pg 85
MSS_CH2_23; For a linear vector space \(S\) over a set of scalars \(R\), for any \(a,\: b \in R\), and for any \(\mathbf{x}, \: \mathbf{y} \in S \), <br> \(a(b\mathbf{x}) = \) {{c1::\( (ab)\mathbf{x} \)}}; VS2, pg 85
MSS_CH2_24; For a linear vector space \(S\) over a set of scalars \(R\), for any \(a,\: b \in R\), and for any \(\mathbf{x}, \: \mathbf{y} \in S \), <br> \( (a + b)\mathbf{x} = \) {{c1::\( a\mathbf{x} + b\mathbf{x}\)}}; VS2, pg 85
MSS_CH2_25; For a linear vector space \(S\) over a set of scalars \(R\), for any \(a,\: b \in R\), and for any \(\mathbf{x}, \: \mathbf{y} \in S \), <br> \( a (\mathbf{x} + \mathbf{y}) = \) {{c1::\( a \mathbf{x} + a\mathbf{y} \)}}; VS2, pg 85
MSS_CH2_26; For a linear vector space \(S\) over a set of scalars \(R\), for any \(a,\: b \in R\), and for any \(\mathbf{x}\), there is a {{c1::multiplicative identity}} element \(1 \in R\) such that {{c2::\(1\mathbf{x} = \mathbf{x}\) }}; VS3, pg 86
MSS_CH2_27; For a linear vector space \(S\) over a set of scalars \(R\), for any \(a,\: b \in R\), and for any \(\mathbf{x}\), there is an element \(0 \in R\) such that {{c1::\(0\mathbf{x} = 0\) }}; VS3, pg 86
MSS_CH2_28; Let \(S\) be a vector space. If \(V \subset S\) such that \(V\) is a vector space, then \(V\) is said to be a {{c1::subspace}} of \(S\).; Definition 2.15, pg 86
MSS_CH2_29; Let \(S\) be a vector space over \(R\) and let \(T \subset S\). A point \(\mathbf{x} \in S\) is said to be a {{c1::linear combination}} of points in \(T\) if there is a {{c1::finite}} set of points {{c2::\(\mathbf{p}_1,\: \mathbf{p}_2,\dots,\:\mathbf{p}_m\) in \(T\)}} and a finite set of scalars {{c2:: \(c_1,\:c_2,\dots,c_m\) in \(R\)}} such that <br> \(\mathbf{x} = \) {{c2:: \(c_1 \mathbf{p}_1,\: c_2 \mathbf{p}_2, \dots, \: c_m \mathbf{p}_m\)}}.; Definition 2.16, pg 88
MSS_CH2_30; Let \(S\) be a vector space and let \(T \subset S\). The set \(T\) is {{c1::linearly independent}} if for each finite nonempty subset of \(T\) (say \( \{\mathbf{p}_1,\: \mathbf{p}_2,\dots,\:\mathbf{p}_m \} \) ) the only set of scalars satisfying the equation <br> {{c2::\(c_1 \mathbf{p}_1,\: c_2 \mathbf{p}_2, \dots, \: c_m \mathbf{p}_m = 0\) }} <br> is the {{c3::trivial}} solution {{c3:: \(c_1 = c_2 = \dots = c_m = 0\)}}.; Definition 2.17, pg 88
MSS_CH2_31; Let \(S\) be a vector space and let \(T \subset S\). The set \(\{\mathbf{p}_1,\: \mathbf{p}_2,\dots,\:\mathbf{p}_m \} \in T\) is {{c1::linearly dependent}} if there exists a set of scalars, not all zero, such that  <br> {{c2::\(c_1 \mathbf{p}_1,\: c_2 \mathbf{p}_2, \dots, \: c_m \mathbf{p}_m = 0\) }}; Definition 2.17, pg 88
MSS_CH2_32; Let \(T\) be a set of vectors in a vector space \(S\). The set of vectors \(V\) that can be reached by {{c2::all possible linear combinations}} of vectors in \(T\) is the {{c1::span}} of the vectors.; Definition 2.18, pg 89
MSS_CH2_33; Let \(T\) be a set of vectors in a vector space \(S\) and let \(V \subset S\) be a subspace. If every vector \(\mathbf{x} \in V\) can be {{c1::written as a linear combination}} of vectors in \(T\), then \(T\) is a {{c1::spanning set}} of \(V\); Definition 2.19, pg 90
MSS_CH2_34; Let \(T\) be a set of vectors in a vector space \(S\) such that \(\text{span}(T) = S\). If \(T\) is {{c1::linearly independent}}, then \(T\) is said to be a {{c1::Hamel basis}} for \(S\).; Definition 2.20, pg 90
# Norms
MSS_CH2_35; Let \(S\) be a vector space with elements \(\mathbf{x}\).  A {{c1::real-valued}} function \( \|\mathbf{x}\| \) is said to be a {{c1::norm}} if \( \|\mathbf{x}\| \) satisfies the following properties:<br> \( \|\mathbf{x}\| \) {{c2::\(\geq\)}} \(0\) for {{c2::any}} \(\mathbf{x} \in S\). <br> \( \|\mathbf{x}\| \) {{c3::\(=\)}} \(0\) {{c3::if and only if}} \( \mathbf{x} = \mathbf{0} \). <br> \( \| a\mathbf{x}\| = \) {{c4::\(|a|\|\mathbf{x}\| \) }} <br> \(\|\mathbf{x} + \mathbf{y} \| \leq\) {{c5::\(\| \mathbf{x} \| + \| \mathbf{y}\| \)}}; Definition 2.22, pg 94 
MSS_CH2_36; The \(l_1\) norm: \(\|\mathbf{x}\|_1 = \) {{c1::\(\sum_{i=1}^{n} |x_i|\)}}.; Example 2.3.1, pg 94
MSS_CH2_37; The \(l_p\) norm: \(\|\mathbf{x}\|_p = \) {{c1::\( \left(\sum_{i=1}^{n} |x_i|^p \right)^{1/p} \)}}.; Example 2.3.1, pg 94
MSS_CH2_38; The \(l_{\infty}\) norm: \(\|\mathbf{x}\|_{\infty} = \) {{c1::\( \max_{i=1,\dots,n}|x_i| \)}}.; Example 2.3.1, pg 94
MSS_CH2_39; The \(L_1\) norm: \(\|x(t)\|_1 = \) {{c1::\(\int_a^b |x(t)|dt \)}}.; Example 2.3.2, pg 95
MSS_CH2_40; The \(L_p\) norm: \(\|x(t)\|_p = \) {{c1::\( \left(\int_a^b |x(t)|^p dt \right)^{1/p} \)}}.; Example 2.3.2, pg 95
MSS_CH2_41; The \(L_{\infty}\) norm: \(\|x(t)\|_{\infty} = \) {{c1::\( \sup_{t\in[a,b]}|x(t)| \)}}.; Example 2.3.2, pg 95
MSS_CH2_42; A {{c1::normed linear space}} is a pair (\(S\), \( \|\cdot \| \)), where \(S\) is a vector space and \( \|\cdot \| \) is a norm defined on \(S\).; Definition 2.23, pg 95
# Inner products
MSS_CH2_43; Let \(S\) be a vector space defined over a scalar field \(R\). An {{c1::inner product}} is a function {{c2::\(\langle \cdot, \cdot \rangle : S \times S \to R \)}} with the following properties: <ol> <li> \( \langle \mathbf{x}, \mathbf{y} \rangle =\) {{c3::\(\overline{\langle \mathbf{y}, \mathbf{x} \rangle} \) }} </li> <li>  \( \langle a\mathbf{x}, \mathbf{y} \rangle =\) {{c4:: \(a\langle \mathbf{y}, \mathbf{x} \rangle \) }} </li> <li>  \( \langle \mathbf{x} + \mathbf{y}, \mathbf{z} \rangle =\) {{c5::\(\langle \mathbf{x}, \mathbf{z} \rangle + \langle \mathbf{y}, \mathbf{z} \rangle \) }} </li> <li> \(\langle \mathbf{x}, \mathbf{x} \rangle \) {{c6::\( > 0\)}} if {{c6::\(\mathbf{x} \neq 0\)}}, and \(\langle \mathbf{x}, \mathbf{x} \rangle \) {{c6::\( = 0\)}} if and only if {{c6::\(\mathbf{x} = 0\)}} </ol>; Definition 2.26, pg 97
MSS_CH2_44; For finite-dimensional vectors \(\mathbf{x},\: \mathbf{y} \in \mathbb{R}^n\) the {{c1::Euclidean inner product}} is \( \langle \mathbf{x}, \mathbf{y} \rangle =\) {{c1:: \(\mathbf{y}^H\mathbf{x} \) }}.; Example 2.4.1, pg 98
MSS_CH2_45; For functions defined over \(\mathbb{R}\), an inner product is \( \langle x(t), y(t) \rangle =\) {{c1::\(\int_{-\infty}^{\infty} x(t) y(t) dt\) }}; Example 2.4.2, pg 98
MSS_CH2_46; We can use the inner product  to produce a special {{c1::norm}}, called the {{c1::induced norm}}. Given an inner product \(\langle \cdot, \cdot \rangle\) in a vector space \(S\), we have the {{c1::induced norm}} \(\|\mathbf{x}\| = \) {{c1::\(\langle \mathbf{x}, \mathbf{x} \rangle^{1/2} \) }}; Section 2.5, pg 99
MSS_CH2_47; The \(l_p\) and \(L_p\) norms are only induced norms when {{c1:: \(p = 2\)}}.; Section 2.5, pg 99
# Cauchy Schwarz
MSS_CH2_48; ({{c1::Cauchy-Schwarz inequality}}) In an inner product space \(S\) with induced norm \(\|\cdot\|\), <br> {{c2::\(| \langle \mathbf{x}, \mathbf{y} \rangle | \)}} \( \leq \) {{c3::\( \|\mathbf{x}\| \|\mathbf{y} \|\)}} for any \(\mathbf{x},\:\mathbf{y} \in S\), with equality if, and only if {{c4::\(\mathbf{y} = a\mathbf{x} \)}} for some {{c4::\(a\)}}.; Theorem 2.4, pg 100
# Direction of vectors
MSS_CH2_49; Vectors \(\mathbf{x}\) and \(\mathbf{y}\) in an inner product space are said to be {{c1::orthogonal}} if {{c2::\(\langle \mathbf{x}, \mathbf{y} \rangle = 0 \)}}.; Definition 2.29, pg 102
# Hilbert and Banach
MSS_CH2_50; A {{c1::complete normed vector}} space is called a {{c2::Banach}} space.; Definition 2.31, pg 106
MSS_CH2_51; A {{c1::complete normed vector}} space with an {{c1::inner product}} (in which the {{c1::norm is the induced norm}}) is called a {{c2::Hilbert}} space.; Definition 2.31, pg 106
MSS_CH2_52; A vector space equipped with an inner product is called an {{c1::inner product space}}.; Definition 2.27, pg 97
# chapter 3
MSS_CH3_01; A matrix \(A\) is said to be positive definite (PD) if {{c1::\(\mathbf{x}^H A \mathbf{x}\ > 0\)}} for all \(\mathbf{x} \neq 0 \); Definition 3.1, pg 134
MSS_CH3_02; A matrix \(A\) is said to be positive semidefinite (PSD) if {{c1::\(\mathbf{x}^H A \mathbf{x}\ \geq 0\)}} for all \(\mathbf{x} \neq 0 \); Definition 3.1, pg 134
MSS_CH3_03; All diagonal elements of a positive definite (or PSD) matrix are {{c1::nonnegative}}; Definition 3.1, pg 134
MSS_CH3_04; A Hermitian matrix \(A\) is PD (or PSD) if and only if all of the eigenvalues are {{c1::nonnegative}}. Hence, a PD matrix has a {{c1::positive determinant}}. Hence, a PD matrix is {{c1::invertable}}; Definition 3.1, pg 134
MSS_CH3_05; A Hermitian matrix \(A\) is PD if and only if all principal minors are {{c1::positive}}.; Definition 3.1, pg 134
MSS_CH3_06; If \(A\) is PD, then the pivots obtained in the {{c1::LU}} factorization are {{c1::positive}}; Definition 3.1, pg 134
MSS_CH3_07; If \(A > 0\) and \(B \geq 0\) (ie \(A\), \(B\) are PD), then \(A + B {{c1:: > 0 }} \).; Definition 3.1, pg 134
MSS_CH3_08; A {{c1::Hermitian}} PD matrix \(A\) can be factored as {{c1::\(A = B^H B\)}}, where \(B\) is full rank. This is a matrix {{c1::square root}}; Definition 3.1, pg 134
MSS_CH3_09; A Grammian matrix \(R\) is always {{c2::positive-semidefinite}}. It is {{c2::positive-definite}} if and only if the vectors \(\mathbf{p}_1\), \(\dots\), \(\mathbf{p}_m\) are {{c1::linearly independent}}.; Theorem 3.1, pg 134
MSS_CH3_10; The {{c1::orthogonality}} principle. Let \(\mathbf{p}_1\), \(\dots\), \(\mathbf{p}_m\) be data vectors in a vector space \(S\). Let \(\mathbf{x} \in S\). In the representation \[\mathbf{x} = \sum_{i=1}^{m} c_i \mathbf{p}_i + \mathbf{e} = \hat{\mathbf{x}} + \mathbf{e},\] the induced norm of the error vector \(\| \mathbf{e} \|\) is minimized when the error \( \mathbf{e} = \mathbf{x} - \hat{\mathbf{x}} \) is {{c1::orthogonal}} to each of the data vectors. i.e. \[ \left\langle \mathbf{x} - \sum_{i=1}^{m} c_i \mathbf{p}_i , \mathbf{p}_j \right\rangle = 0\] for \(j = 1,2,\dots, m\).; Theorem 3.2, pg 135
MSS_CH3_11; The optimal (least-squares) coefficients \(\mathbf{c}\) are \[\mathbf{c} = {{c1:: (A^HA)^{-1}A^H }} \mathbf{x}\].; Equation 3.19, pg 139
